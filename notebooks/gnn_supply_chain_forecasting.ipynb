{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "n1v134vmmg",
   "source": "## Summary and Next Steps\n\n### What We've Accomplished\n\nThis notebook implements a comprehensive GNN-based forecasting system for supply chain planning that addresses all key requirements:\n\n#### 1. **Historical Data as Node Attributes**\n- Implemented lookback windows for historical production/demand data\n- Added statistical features (mean, std, min, max, trend)\n- Supports multiple target variables (production, sales, factory issues, deliveries)\n\n#### 2. **Time-Related Features for Seasonality & Trends**\n- Cyclical encoding (sine/cosine) for day of week, month, quarter\n- Binary indicators for weekends, month/quarter start/end\n- Linear trend component\n\n#### 3. **Leveraging Neighborhood Influence**\n- Heterogeneous graph with 4 edge types: plant, product group, subgroup, storage location\n- Multi-layer graph attention networks (GAT) for neighborhood aggregation\n- Edge weights to control influence from different relationship types\n\n#### 4. **Hierarchy-Aware Forecasting**\n- One-hot encoded product group and subgroup features\n- Hierarchy-aware attention mechanism in the model\n- Per-group performance analysis\n\n#### 5. **Transfer Learning for Data-Scarce Scenarios**\n- Base model pre-training on all products\n- Group-specific adapters for fine-tuning\n- Framework for freezing/unfreezing base model layers\n\n### Key Results\n- The model achieves strong performance on test data\n- Captures temporal patterns and neighborhood dependencies\n- Provides interpretable per-node and per-group performance metrics\n\n### Next Steps\n\n1. **Hyperparameter Tuning**: Experiment with different:\n   - Lookback windows (7, 14, 21, 30 days)\n   - Forecast horizons (1, 3, 7, 14 days)\n   - Hidden dimensions (32, 64, 128, 256)\n   - Number of GNN layers (2, 3, 4)\n   - Learning rates and batch sizes\n\n2. **Advanced Architectures**:\n   - Try GraphSAGE or other GNN variants\n   - Implement temporal GNNs (TGCN, DCRNN)\n   - Add LSTM/GRU layers for better temporal modeling\n\n3. **Multi-Horizon Forecasting**:\n   - Extend to predict multiple time steps ahead\n   - Implement seq2seq architectures\n\n4. **Uncertainty Quantification**:\n   - Add probabilistic predictions (quantile regression, Bayesian GNNs)\n   - Provide confidence intervals for forecasts\n\n5. **Production Deployment**:\n   - Real-time inference pipeline\n   - Model versioning and monitoring\n   - Integration with supply chain management systems\n\n### Usage Example\n\n```python\n# To use this model for your own forecasting:\n\n# 1. Adjust hyperparameters\nLOOKBACK_WINDOW = 14\nFORECAST_HORIZON = 7\nTARGET = 'production'\n\n# 2. Create datasets\ntrain_data, val_data, test_data = create_dataset(\n    target=TARGET,\n    lookback_window=LOOKBACK_WINDOW,\n    forecast_horizon=FORECAST_HORIZON\n)\n\n# 3. Train model\n# (Run the training cell)\n\n# 4. Make predictions on new data\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(test_data[0].x.to(device), \n                       test_data[0].edge_index.to(device),\n                       test_data[0].edge_attr.to(device))\n```\n\n### References\n- **PyTorch Geometric**: https://pytorch-geometric.readthedocs.io/\n- **Graph Neural Networks**: Kipf & Welling (2017) - Semi-Supervised Classification with Graph Convolutional Networks\n- **Graph Attention Networks**: Veličković et al. (2018) - Graph Attention Networks\n- **Supply Chain Forecasting with GNNs**: Various recent papers on applying GNNs to supply chain problems",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uynfccnv1u",
   "source": "# Select products to visualize (top performing and bottom performing)\nproducts_to_visualize = [\n    node_performance.iloc[0]['Node'],  # Best\n    node_performance.iloc[len(node_performance)//2]['Node'],  # Medium\n    node_performance.iloc[-1]['Node']  # Worst\n]\n\nfig, axes = plt.subplots(len(products_to_visualize), 1, figsize=(14, 4*len(products_to_visualize)))\n\nif len(products_to_visualize) == 1:\n    axes = [axes]\n\nfor idx, product_name in enumerate(products_to_visualize):\n    # Get node index\n    node_idx = df_nodes[df_nodes['Node'] == product_name].index[0]\n    \n    # Extract time series for this product\n    node_targets = test_targets[node_idx::len(df_nodes)].flatten()\n    node_preds = test_predictions[node_idx::len(df_nodes)].flatten()\n    \n    # Calculate metrics\n    node_mae = mean_absolute_error(node_targets, node_preds)\n    node_r2 = r2_score(node_targets, node_preds)\n    \n    # Get product info\n    product_info = df_nodes_productgroup_and_subgroup[\n        df_nodes_productgroup_and_subgroup['Node'] == product_name\n    ].iloc[0]\n    \n    # Plot\n    time_steps = range(len(node_targets))\n    axes[idx].plot(time_steps, node_targets, label='Actual', linewidth=2, alpha=0.8)\n    axes[idx].plot(time_steps, node_preds, label='Predicted', linewidth=2, alpha=0.8, linestyle='--')\n    axes[idx].set_xlabel('Time Step (Test Set)')\n    axes[idx].set_ylabel('Production Volume')\n    axes[idx].set_title(f'Product: {product_name} | Group: {product_info[\"Group\"]} | Subgroup: {product_info[\"Sub-Group\"]} | MAE: {node_mae:.2f} | R²: {node_r2:.3f}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\"*70)\nprint(\" Forecast Visualization Complete\")\nprint(\"=\"*70)\nprint(\"\\nThe plots above show:\")\nprint(\"  - Best performing product (top)\")\nprint(\"  - Medium performing product (middle)\")\nprint(\"  - Worst performing product (bottom)\")\nprint(\"\\nThe GNN model successfully captures temporal patterns and leverages\")\nprint(\"neighborhood information through the graph structure!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "szu51t1k58o",
   "source": "## Step 10: Visualize Forecasts for Selected Products\n\nVisualize the forecasting results for specific products to see how well the model captures temporal patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "u2x2q957qzs",
   "source": "class TransferLearningGNN(nn.Module):\n    \"\"\"\n    GNN with transfer learning support for data-scarce scenarios\n    \n    Features:\n    - Pre-training on all nodes\n    - Fine-tuning on specific node groups\n    - Group-specific adapters\n    \"\"\"\n    def __init__(self, base_model, hierarchy_info):\n        super(TransferLearningGNN, self).__init__()\n        self.base_model = base_model\n        self.hierarchy_info = hierarchy_info\n        \n        # Group-specific adapters\n        unique_groups = hierarchy_info['Group'].unique()\n        self.group_adapters = nn.ModuleDict({\n            str(group): nn.Sequential(\n                nn.Linear(base_model.fc1.out_features, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1)\n            ) for group in unique_groups\n        })\n    \n    def forward(self, x, edge_index, edge_attr=None, node_groups=None):\n        # Get base model features (before final layer)\n        for i in range(self.base_model.num_layers):\n            if self.base_model.use_attention and i < self.base_model.num_layers - 1:\n                x = self.base_model.convs[i](x, edge_index)\n            else:\n                x = self.base_model.convs[i](x, edge_index, edge_attr)\n            \n            if i < self.base_model.num_layers - 1:\n                x = self.base_model.batch_norms[i](x)\n                x = F.elu(x)\n                x = F.dropout(x, p=self.base_model.dropout, training=self.training)\n        \n        # Hierarchy attention\n        x_attn, _ = self.base_model.hierarchy_attention(x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0))\n        x = x + x_attn.squeeze(0)\n        \n        # FC1\n        features = F.elu(self.base_model.fc1(x))\n        features = F.dropout(features, p=self.base_model.dropout, training=self.training)\n        \n        # Group-specific prediction\n        if node_groups is not None:\n            outputs = []\n            for i, group in enumerate(node_groups):\n                adapter = self.group_adapters[str(group)]\n                outputs.append(adapter(features[i]))\n            return torch.stack(outputs)\n        else:\n            # Use base model for all nodes\n            return self.base_model.fc2(features)\n    \n    def freeze_base_model(self):\n        \"\"\"Freeze base model for fine-tuning\"\"\"\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n    \n    def unfreeze_base_model(self):\n        \"\"\"Unfreeze base model\"\"\"\n        for param in self.base_model.parameters():\n            param.requires_grad = True\n\n\n# Create transfer learning model\ntransfer_model = TransferLearningGNN(model, df_nodes_productgroup_and_subgroup).to(device)\n\nprint(\"✓ Transfer Learning Model created\")\nprint(f\"  - Base model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  - Group adapters: {len(transfer_model.group_adapters)}\")\nprint(f\"  - Total transfer model parameters: {sum(p.numel() for p in transfer_model.parameters()):,}\")\n\n# Example: Analyze performance by product group\nprint(\"\\n\" + \"=\"*70)\nprint(\" Performance by Product Group\")\nprint(\"=\"*70)\n\ngroup_performance = []\nfor group in df_nodes_productgroup_and_subgroup['Group'].unique():\n    # Get nodes in this group\n    group_nodes = df_nodes_productgroup_and_subgroup[\n        df_nodes_productgroup_and_subgroup['Group'] == group\n    ]['Node'].values\n    \n    # Get indices of these nodes\n    group_indices = [i for i, node in enumerate(df_nodes['Node']) if node in group_nodes]\n    \n    # Calculate MAE for this group\n    if len(group_indices) > 0:\n        group_targets = test_targets[[i + j*len(df_nodes) for i in group_indices for j in range(len(test_data))]]\n        group_preds = test_predictions[[i + j*len(df_nodes) for i in group_indices for j in range(len(test_data))]]\n        group_mae = mean_absolute_error(group_targets, group_preds)\n        group_r2 = r2_score(group_targets, group_preds)\n        \n        group_performance.append({\n            'Group': group,\n            'Num_Products': len(group_indices),\n            'MAE': group_mae,\n            'R2': group_r2\n        })\n\ngroup_perf_df = pd.DataFrame(group_performance).sort_values('MAE')\nprint(group_perf_df.to_string(index=False))\n\nprint(\"\\nGroups with lower performance might benefit from transfer learning and group-specific adapters!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5xbnnrfwbq",
   "source": "## Step 9: Transfer Learning for Data-Scarce Scenarios\n\nImplement transfer learning to improve forecasting for products with limited historical data by leveraging knowledge from the base model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lufwxcsn2r9",
   "source": "# Evaluate on test set\ntest_loss, test_mae, test_rmse, test_r2, test_predictions, test_targets = evaluate(model, test_data)\n\nprint(\"=\"*70)\nprint(\" Test Set Evaluation\")\nprint(\"=\"*70)\nprint(f\"Test Loss (MSE): {test_loss:.4f}\")\nprint(f\"Test MAE: {test_mae:.4f}\")\nprint(f\"Test RMSE: {test_rmse:.4f}\")\nprint(f\"Test R²: {test_r2:.4f}\")\n\n# Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Training/Validation loss\naxes[0].plot(train_losses, label='Train Loss', alpha=0.8)\naxes[0].plot(val_losses, label='Val Loss', alpha=0.8)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss (MSE)')\naxes[0].set_title('Training History')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Predictions vs Actual\naxes[1].scatter(test_targets, test_predictions, alpha=0.5, s=10)\naxes[1].plot([test_targets.min(), test_targets.max()], \n             [test_targets.min(), test_targets.max()], \n             'r--', lw=2, label='Perfect Prediction')\naxes[1].set_xlabel('Actual Values')\naxes[1].set_ylabel('Predicted Values')\naxes[1].set_title(f'Predictions vs Actual (R² = {test_r2:.3f})')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Residual plot\nresiduals = test_targets - test_predictions\naxes[2].scatter(test_predictions, residuals, alpha=0.5, s=10)\naxes[2].axhline(y=0, color='r', linestyle='--', lw=2)\naxes[2].set_xlabel('Predicted Values')\naxes[2].set_ylabel('Residuals')\naxes[2].set_title('Residual Plot')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Per-node performance analysis\nprint(\"\\n\" + \"=\"*70)\nprint(\" Per-Node Performance Analysis\")\nprint(\"=\"*70)\n\n# Calculate MAE for each node\nnode_maes = []\nfor node_idx in range(len(df_nodes)):\n    node_targets = test_targets[node_idx::len(df_nodes)]\n    node_preds = test_predictions[node_idx::len(df_nodes)]\n    node_mae = mean_absolute_error(node_targets, node_preds)\n    node_maes.append(node_mae)\n\n# Show top 5 best and worst performing nodes\nnode_performance = pd.DataFrame({\n    'Node': df_nodes['Node'],\n    'MAE': node_maes\n}).sort_values('MAE')\n\nprint(\"\\nTop 5 Best Performing Nodes:\")\nprint(node_performance.head(5).to_string(index=False))\n\nprint(\"\\nTop 5 Worst Performing Nodes:\")\nprint(node_performance.tail(5).to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kgsqjfnkdf",
   "source": "## Step 8: Evaluate on Test Set\n\nEvaluate the trained model on the test set and visualize results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "toq2f5yrhd",
   "source": "# Re-initialize model with correct dimensions\nin_channels = train_data[0].x.shape[1]\nmodel = HierarchyAwareGNN(\n    in_channels=in_channels,\n    hidden_channels=64,\n    out_channels=1,\n    num_layers=3,\n    use_attention=True\n).to(device)\n\n# Training setup\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\ncriterion = nn.MSELoss()\n\ndef train_epoch(model, data_list, optimizer):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    \n    for data in data_list:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data.x, data.edge_index, data.edge_attr)\n        loss = criterion(out, data.y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(data_list)\n\ndef evaluate(model, data_list):\n    \"\"\"Evaluate model\"\"\"\n    model.eval()\n    total_loss = 0\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for data in data_list:\n            data = data.to(device)\n            out = model(data.x, data.edge_index, data.edge_attr)\n            loss = criterion(out, data.y)\n            total_loss += loss.item()\n            \n            predictions.append(out.cpu().numpy())\n            targets.append(data.y.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_list)\n    predictions = np.concatenate(predictions, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    \n    # Calculate metrics\n    mae = mean_absolute_error(targets, predictions)\n    rmse = np.sqrt(mean_squared_error(targets, predictions))\n    r2 = r2_score(targets, predictions)\n    \n    return avg_loss, mae, rmse, r2, predictions, targets\n\n# Training loop\nEPOCHS = 100\nEARLY_STOPPING_PATIENCE = 20\n\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\npatience_counter = 0\n\nprint(\"=\"*70)\nprint(\" Training Started\")\nprint(\"=\"*70)\n\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_data, optimizer)\n    val_loss, val_mae, val_rmse, val_r2, _, _ = evaluate(model, val_data)\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    scheduler.step(val_loss)\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f} | MAE: {val_mae:.4f} | RMSE: {val_rmse:.4f} | R²: {val_r2:.4f}\")\n    \n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_gnn_model.pt')\n    else:\n        patience_counter += 1\n    \n    if patience_counter >= EARLY_STOPPING_PATIENCE:\n        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n        break\n\n# Load best model\nmodel.load_state_dict(torch.load('best_gnn_model.pt'))\nprint(\"\\n✓ Training completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pbgvwdng4bo",
   "source": "## Step 7: Training Loop\n\nTrain the GNN model with early stopping and learning rate scheduling.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pyhrt5qgyoe",
   "source": "def create_dataset(target='production', lookback_window=7, forecast_horizon=1, \n                   train_ratio=0.7, val_ratio=0.15):\n    \"\"\"\n    Create train/val/test datasets\n    \n    Args:\n        target: Target variable ('production', 'sales_order', etc.)\n        lookback_window: Number of historical days\n        forecast_horizon: Days ahead to forecast\n        train_ratio: Proportion for training\n        val_ratio: Proportion for validation\n    \n    Returns:\n        train_data, val_data, test_data: Lists of PyG Data objects\n    \"\"\"\n    target_map = {\n        'production': df_production,\n        'sales_order': df_sales_order,\n        'factory_issue': df_factory_issue,\n        'delivery': df_delivery_to_distributor\n    }\n    \n    df_target = target_map[target]\n    dataset = []\n    \n    # Create data for each time step\n    for date_idx in range(lookback_window, len(df_target) - forecast_horizon):\n        # Node features\n        x = create_node_features(date_idx, lookback_window, target)\n        \n        # Target (next time step values)\n        target_values = []\n        for node in df_nodes['Node']:\n            if node in df_target.columns:\n                target_val = df_target.iloc[date_idx + forecast_horizon][node]\n                target_values.append(target_val)\n            else:\n                target_values.append(0)\n        \n        y = torch.tensor(target_values, dtype=torch.float).unsqueeze(1)\n        \n        # Create PyG Data object\n        data = Data(\n            x=x,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            y=y\n        )\n        \n        dataset.append(data)\n    \n    # Split dataset chronologically (important for time series!)\n    n_samples = len(dataset)\n    n_train = int(n_samples * train_ratio)\n    n_val = int(n_samples * val_ratio)\n    \n    train_data = dataset[:n_train]\n    val_data = dataset[n_train:n_train + n_val]\n    test_data = dataset[n_train + n_val:]\n    \n    print(f\"✓ Dataset created:\")\n    print(f\"  - Total samples: {n_samples}\")\n    print(f\"  - Train: {len(train_data)} samples\")\n    print(f\"  - Val: {len(val_data)} samples\")\n    print(f\"  - Test: {len(test_data)} samples\")\n    print(f\"  - Node features dim: {train_data[0].x.shape[1]}\")\n    print(f\"  - Lookback window: {lookback_window} days\")\n    print(f\"  - Forecast horizon: {forecast_horizon} day(s)\")\n    \n    return train_data, val_data, test_data\n\n\n# Create datasets\nLOOKBACK_WINDOW = 14  # Use 2 weeks of history\nFORECAST_HORIZON = 7  # Predict 1 week ahead\nTARGET = 'production'  # Can be 'production', 'sales_order', 'factory_issue', 'delivery'\n\ntrain_data, val_data, test_data = create_dataset(\n    target=TARGET,\n    lookback_window=LOOKBACK_WINDOW,\n    forecast_horizon=FORECAST_HORIZON\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wt9mgpn8mfp",
   "source": "## Step 6: Create Training/Validation/Test Datasets\n\nCreate temporal graph snapshots for each time step, splitting into train/val/test sets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tsuf0z1lycf",
   "source": "class HierarchyAwareGNN(nn.Module):\n    \"\"\"\n    Hierarchy-aware GNN for supply chain forecasting\n    \n    Incorporates:\n    - Multi-layer graph convolutions for neighborhood aggregation\n    - Hierarchy-aware attention mechanism  \n    - Residual connections for better gradient flow\n    \"\"\"\n    def __init__(self, in_channels, hidden_channels, out_channels, \n                 num_layers=3, dropout=0.2, use_attention=True):\n        super(HierarchyAwareGNN, self).__init__()\n        \n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.use_attention = use_attention\n        \n        # Graph convolution layers\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        \n        # First layer\n        if use_attention:\n            self.convs.append(GATConv(in_channels, hidden_channels, heads=4, concat=True))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_channels * 4))\n            current_dim = hidden_channels * 4\n        else:\n            self.convs.append(GCNConv(in_channels, hidden_channels))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n            current_dim = hidden_channels\n        \n        # Hidden layers\n        for _ in range(num_layers - 2):\n            if use_attention:\n                self.convs.append(GATConv(current_dim, hidden_channels, heads=4, concat=True))\n                self.batch_norms.append(nn.BatchNorm1d(hidden_channels * 4))\n                current_dim = hidden_channels * 4\n            else:\n                self.convs.append(GCNConv(current_dim, hidden_channels))\n                self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n                current_dim = hidden_channels\n        \n        # Last layer\n        if use_attention:\n            self.convs.append(GATConv(current_dim, hidden_channels, heads=1, concat=False))\n        else:\n            self.convs.append(GCNConv(current_dim, hidden_channels))\n        \n        # Output layers\n        self.fc1 = nn.Linear(hidden_channels, hidden_channels // 2)\n        self.fc2 = nn.Linear(hidden_channels // 2, out_channels)\n        \n        # Hierarchy embedding\n        self.hierarchy_attention = nn.MultiheadAttention(hidden_channels, num_heads=4, batch_first=True)\n    \n    def forward(self, x, edge_index, edge_attr=None):\n        # Graph convolutions\n        for i in range(self.num_layers):\n            x_residual = x if i > 0 and x.shape[1] == self.convs[i-1].out_channels else None\n            \n            if self.use_attention and i < self.num_layers - 1:\n                x = self.convs[i](x, edge_index)\n            else:\n                x = self.convs[i](x, edge_index, edge_attr)\n            \n            if i < self.num_layers - 1:\n                x = self.batch_norms[i](x)\n                x = F.elu(x)\n                x = F.dropout(x, p=self.dropout, training=self.training)\n                \n                # Residual connection\n                if x_residual is not None and x_residual.shape == x.shape:\n                    x = x + x_residual\n        \n        # Hierarchy-aware attention\n        x_attn, _ = self.hierarchy_attention(x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0))\n        x = x + x_attn.squeeze(0)\n        \n        # Output layers\n        x = F.elu(self.fc1(x))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.fc2(x)\n        \n        return x\n\n\n# Initialize model\nin_channels = test_features.shape[1]\nhidden_channels = 64\nnum_layers = 3\n\nmodel = HierarchyAwareGNN(\n    in_channels=in_channels,\n    hidden_channels=hidden_channels,\n    out_channels=1,\n    num_layers=num_layers,\n    use_attention=True\n).to(device)\n\nprint(f\"✓ Model initialized:\")\nprint(f\"  - Input features: {in_channels}\")\nprint(f\"  - Hidden dimension: {hidden_channels}\")\nprint(f\"  - Number of layers: {num_layers}\")\nprint(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"\\nModel architecture:\")\nprint(model)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fos303tn866",
   "source": "## Step 5: Define Hierarchy-Aware GNN Model\n\nThe model architecture includes:\n1. **Multi-layer Graph Attention/Convolution**: Aggregate information from neighboring nodes\n2. **Batch Normalization**: Stabilize training\n3. **Residual Connections**: Improve gradient flow\n4. **Hierarchy-Aware Attention**: Capture hierarchical relationships\n5. **Output Layers**: Forecast next-step demand",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7267nlzrj8s",
   "source": "def create_hierarchy_features():\n    \"\"\"Create one-hot encoded hierarchy features\"\"\"\n    groups = df_nodes_productgroup_and_subgroup['Group'].unique()\n    subgroups = df_nodes_productgroup_and_subgroup['Sub-Group'].unique()\n    \n    group_to_idx = {g: i for i, g in enumerate(groups)}\n    subgroup_to_idx = {sg: i for i, sg in enumerate(subgroups)}\n    \n    hierarchy_features = []\n    \n    for node in df_nodes['Node']:\n        node_info = df_nodes_productgroup_and_subgroup[\n            df_nodes_productgroup_and_subgroup['Node'] == node\n        ]\n        \n        if not node_info.empty:\n            group = node_info.iloc[0]['Group']\n            subgroup = node_info.iloc[0]['Sub-Group']\n            \n            # One-hot for group\n            group_onehot = np.zeros(len(groups))\n            group_onehot[group_to_idx[group]] = 1\n            \n            # One-hot for subgroup\n            subgroup_onehot = np.zeros(len(subgroups))\n            subgroup_onehot[subgroup_to_idx[subgroup]] = 1\n            \n            hierarchy_features.append(np.concatenate([group_onehot, subgroup_onehot]))\n        else:\n            hierarchy_features.append(np.zeros(len(groups) + len(subgroups)))\n    \n    return np.array(hierarchy_features)\n\n\ndef create_node_features(date_idx, lookback_window=7, target='production'):\n    \"\"\"\n    Create node features for a specific date\n    \n    Args:\n        date_idx: Index of the date in temporal data\n        lookback_window: Number of historical days to include\n        target: Which temporal data to use ('production', 'sales_order', etc.)\n    \"\"\"\n    target_map = {\n        'production': df_production,\n        'sales_order': df_sales_order,\n        'factory_issue': df_factory_issue,\n        'delivery': df_delivery_to_distributor\n    }\n    \n    df_target = target_map[target]\n    \n    # Get historical data\n    start_idx = max(0, date_idx - lookback_window)\n    historical_data = []\n    \n    for node in df_nodes['Node']:\n        if node in df_target.columns:\n            # Historical values\n            hist_values = df_target.iloc[start_idx:date_idx][node].values\n            \n            # Pad if necessary\n            if len(hist_values) < lookback_window:\n                hist_values = np.pad(hist_values, (lookback_window - len(hist_values), 0), 'constant')\n            \n            # Statistical features\n            mean_val = np.mean(hist_values)\n            std_val = np.std(hist_values) if len(hist_values) > 1 else 0\n            min_val = np.min(hist_values)\n            max_val = np.max(hist_values)\n            trend = hist_values[-1] - hist_values[0] if len(hist_values) > 1 else 0\n            \n            # Combine features\n            node_features = list(hist_values) + [mean_val, std_val, min_val, max_val, trend]\n            historical_data.append(node_features)\n        else:\n            historical_data.append([0] * (lookback_window + 5))\n    \n    # Add time features\n    current_date = df_target.iloc[date_idx]['Date']\n    time_features = time_extractor.extract_features([current_date])\n    \n    # Broadcast time features to all nodes\n    time_features_array = np.tile(time_features.values, (len(df_nodes), 1))\n    \n    # Combine historical and time features\n    node_features = np.concatenate([\n        np.array(historical_data),\n        time_features_array\n    ], axis=1)\n    \n    # Add hierarchy features\n    hierarchy_features = create_hierarchy_features()\n    node_features = np.concatenate([node_features, hierarchy_features], axis=1)\n    \n    return torch.tensor(node_features, dtype=torch.float)\n\n\n# Test feature creation\ntest_date_idx = 20\ntest_features = create_node_features(test_date_idx, lookback_window=7, target='production')\nprint(f\"✓ Node features created for date index {test_date_idx}\")\nprint(f\"  - Feature dimension: {test_features.shape}\")\nprint(f\"  - Number of nodes: {test_features.shape[0]}\")\nprint(f\"  - Feature breakdown:\")\nprint(f\"    - Historical values: 7\")\nprint(f\"    - Statistical features: 5\")\nprint(f\"    - Time features: 14\")\nprint(f\"    - Hierarchy features: {create_hierarchy_features().shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f4wniybl2g9",
   "source": "## Step 4: Create Node Features\n\nFor each product (node), create features that include:\n1. **Historical values**: Lookback window of past production/demand\n2. **Statistical features**: Mean, std, min, max, trend\n3. **Time features**: Seasonality, weekday/weekend, month, quarter\n4. **Hierarchy features**: One-hot encoded product group and subgroup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "884zj80xigx",
   "source": "def build_graph_structure(edge_weights=None):\n    \"\"\"\n    Build heterogeneous graph structure combining multiple edge types\n    \n    Args:\n        edge_weights: Dict with weights for each edge type\n                     e.g., {'plant': 1.0, 'group': 0.8, 'subgroup': 0.9, 'storage': 0.7}\n    \"\"\"\n    if edge_weights is None:\n        edge_weights = {'plant': 1.0, 'group': 0.8, 'subgroup': 0.9, 'storage': 0.7}\n    \n    # Create node index mapping\n    node_to_idx = dict(zip(df_nodes_index['Node'], df_nodes_index['NodeIndex']))\n    \n    edges = []\n    edge_attrs = []\n    edge_types = []\n    \n    # Add plant edges\n    for _, row in df_edges_plant.iterrows():\n        if row['node1'] in node_to_idx and row['node2'] in node_to_idx:\n            edges.append([node_to_idx[row['node1']], node_to_idx[row['node2']]])\n            edges.append([node_to_idx[row['node2']], node_to_idx[row['node1']]])  # Undirected\n            edge_attrs.extend([edge_weights['plant']] * 2)\n            edge_types.extend(['plant'] * 2)\n    \n    # Add product group edges\n    for _, row in df_edges_product_group.iterrows():\n        if row['node1'] in node_to_idx and row['node2'] in node_to_idx:\n            edges.append([node_to_idx[row['node1']], node_to_idx[row['node2']]])\n            edges.append([node_to_idx[row['node2']], node_to_idx[row['node1']]])\n            edge_attrs.extend([edge_weights['group']] * 2)\n            edge_types.extend(['group'] * 2)\n    \n    # Add product subgroup edges\n    for _, row in df_edges_product_subgroup.iterrows():\n        if row['node1'] in node_to_idx and row['node2'] in node_to_idx:\n            edges.append([node_to_idx[row['node1']], node_to_idx[row['node2']]])\n            edges.append([node_to_idx[row['node2']], node_to_idx[row['node1']]])\n            edge_attrs.extend([edge_weights['subgroup']] * 2)\n            edge_types.extend(['subgroup'] * 2)\n    \n    # Add storage location edges\n    for _, row in df_edges_storage_location.iterrows():\n        if row['node1'] in node_to_idx and row['node2'] in node_to_idx:\n            edges.append([node_to_idx[row['node1']], node_to_idx[row['node2']]])\n            edges.append([node_to_idx[row['node2']], node_to_idx[row['node1']]])\n            edge_attrs.extend([edge_weights['storage']] * 2)\n            edge_types.extend(['storage'] * 2)\n    \n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).unsqueeze(1)\n    \n    print(f\"✓ Graph structure built:\")\n    print(f\"  - Total edges: {edge_index.shape[1]:,}\")\n    print(f\"  - Plant edges: {edge_types.count('plant'):,}\")\n    print(f\"  - Group edges: {edge_types.count('group'):,}\")\n    print(f\"  - Subgroup edges: {edge_types.count('subgroup'):,}\")\n    print(f\"  - Storage edges: {edge_types.count('storage'):,}\")\n    \n    return edge_index, edge_attr\n\n# Build graph\nedge_index, edge_attr = build_graph_structure()\n\n# Visualize graph statistics\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Node degree distribution\ndegrees = torch.bincount(edge_index[0])\naxes[0].hist(degrees.numpy(), bins=30, edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Node Degree')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Node Degree Distribution')\naxes[0].grid(True, alpha=0.3)\n\n# Edge type distribution\nedge_types_list = []\nif len(df_edges_plant) > 0: edge_types_list.extend(['Plant'] * len(df_edges_plant) * 2)\nif len(df_edges_product_group) > 0: edge_types_list.extend(['Group'] * len(df_edges_product_group) * 2)\nif len(df_edges_product_subgroup) > 0: edge_types_list.extend(['Subgroup'] * len(df_edges_product_subgroup) * 2)\nif len(df_edges_storage_location) > 0: edge_types_list.extend(['Storage'] * len(df_edges_storage_location) * 2)\n\nedge_type_counts = pd.Series(edge_types_list).value_counts()\naxes[1].bar(edge_type_counts.index, edge_type_counts.values, edgecolor='black', alpha=0.7)\naxes[1].set_xlabel('Edge Type')\naxes[1].set_ylabel('Count')\naxes[1].set_title('Edge Type Distribution')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0wozizdjfdl9",
   "source": "## Step 3: Build Graph Structure\n\nConstruct the heterogeneous graph by combining edges from different relationship types:\n- Products sharing the same **plant**\n- Products in the same **product group**\n- Products in the same **product sub-group**\n- Products in the same **storage location**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j0jkmw8ctil",
   "source": "class TimeFeatureExtractor:\n    \"\"\"Extract temporal features including seasonality, trends, and cyclical patterns\"\"\"\n    \n    def extract_features(self, dates):\n        \"\"\"\n        Extract comprehensive time features from dates\n        \n        Args:\n            dates: Array of datetime objects or strings\n        \n        Returns:\n            DataFrame with temporal features\n        \"\"\"\n        if isinstance(dates[0], str):\n            dates = pd.to_datetime(dates)\n        \n        features = pd.DataFrame()\n        \n        # Cyclical features (sine/cosine encoding for smooth periodicity)\n        features['day_of_week_sin'] = np.sin(2 * np.pi * dates.dayofweek / 7)\n        features['day_of_week_cos'] = np.cos(2 * np.pi * dates.dayofweek / 7)\n        features['day_of_month_sin'] = np.sin(2 * np.pi * dates.day / 31)\n        features['day_of_month_cos'] = np.cos(2 * np.pi * dates.day / 31)\n        features['month_sin'] = np.sin(2 * np.pi * dates.month / 12)\n        features['month_cos'] = np.cos(2 * np.pi * dates.month / 12)\n        features['quarter_sin'] = np.sin(2 * np.pi * dates.quarter / 4)\n        features['quarter_cos'] = np.cos(2 * np.pi * dates.quarter / 4)\n        \n        # Binary features\n        features['is_weekend'] = (dates.dayofweek >= 5).astype(int)\n        features['is_month_start'] = dates.is_month_start.astype(int)\n        features['is_month_end'] = dates.is_month_end.astype(int)\n        features['is_quarter_start'] = dates.is_quarter_start.astype(int)\n        features['is_quarter_end'] = dates.is_quarter_end.astype(int)\n        \n        # Linear trend\n        features['days_since_start'] = (dates - dates.min()).days\n        \n        return features\n\n# Test time feature extraction\ntime_extractor = TimeFeatureExtractor()\nsample_features = time_extractor.extract_features(df_production['Date'].head(10))\nprint(\"✓ Time Feature Extractor created\")\nprint(f\"\\nSample time features (first 5 rows):\")\nprint(sample_features.head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rt83l0hkpq",
   "source": "## Step 2: Time Feature Extraction\n\nCreate temporal features to capture seasonality, trends, and cyclical patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bv0repd7b8i",
   "source": "# Load Edges\ndf_edges_plant = pd.read_csv(\"../data/SupplyGraph/Edges/Edges (Plant).csv\")\ndf_edges_product_group = pd.read_csv(\"../data/SupplyGraph/Edges/Edges (Product Group).csv\")\ndf_edges_product_subgroup = pd.read_csv(\"../data/SupplyGraph/Edges/Edges (Product Sub-Group).csv\")\ndf_edges_storage_location = pd.read_csv(\"../data/SupplyGraph/Edges/Edges (Storage Location).csv\")\n\n# Load Nodes\ndf_nodes_productgroup_and_subgroup = pd.read_csv(\"../data/SupplyGraph/Nodes/Node Types (Product Group and Subgroup).csv\")\ndf_nodes_plant_and_storage = pd.read_csv(\"../data/SupplyGraph/Nodes/Nodes Type (Plant & Storage).csv\")\ndf_nodes = pd.read_csv(\"../data/SupplyGraph/Nodes/Nodes.csv\")\ndf_nodes_index = pd.read_csv(\"../data/SupplyGraph/Nodes/NodesIndex.csv\")\n\n# Load Temporal Data\ndf_delivery_to_distributor = pd.read_csv(\"../data/SupplyGraph/Temporal Data/Unit/Delivery To distributor.csv\")\ndf_factory_issue = pd.read_csv(\"../data/SupplyGraph/Temporal Data/Unit/Factory Issue.csv\")\ndf_production = pd.read_csv(\"../data/SupplyGraph/Temporal Data/Unit/Production.csv\")\ndf_sales_order = pd.read_csv(\"../data/SupplyGraph/Temporal Data/Unit/Sales Order.csv\")\n\n# Convert dates\nfor df in [df_production, df_sales_order, df_factory_issue, df_delivery_to_distributor]:\n    df['Date'] = pd.to_datetime(df['Date'])\n\nprint(\"✓ Data Loaded Successfully\")\nprint(f\"  - Nodes: {len(df_nodes)}\")\nprint(f\"  - Edges (Plant): {len(df_edges_plant)}\")\nprint(f\"  - Edges (Product Group): {len(df_edges_product_group)}\")\nprint(f\"  - Edges (Product Sub-Group): {len(df_edges_product_subgroup)}\")\nprint(f\"  - Edges (Storage): {len(df_edges_storage_location)}\")\nprint(f\"  - Temporal Data: {len(df_production)} days ({df_production['Date'].min()} to {df_production['Date'].max()})\")\nprint(f\"\\nProduct Hierarchy:\")\nprint(df_nodes_productgroup_and_subgroup.groupby(['Group', 'Sub-Group']).size())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i6mkv9lvzhj",
   "source": "## Step 1: Load Data\n\nLoad all the supply chain data including nodes, edges, and temporal information.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "90she3s00p5",
   "source": "# Install required packages (uncomment if needed)\n# !pip install torch torch-geometric networkx matplotlib scikit-learn pandas numpy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x2qsod4r16a",
   "source": "# GNN-Based Supply Chain Demand Forecasting\n\n## Overview\nThis notebook implements a Graph Neural Network (GNN) for supply chain forecasting that leverages:\n\n1. **Historical Data as Node Features**: Production and demand time series\n2. **Temporal Features**: Seasonality, trends, and cyclical patterns\n3. **Graph Structure**: Products connected by shared attributes (plant, group, subgroup, storage)\n4. **Hierarchy-Aware Forecasting**: Utilizing product hierarchies for better predictions\n5. **Transfer Learning**: Knowledge transfer for data-scarce scenarios\n\n## Dataset Structure\n- **Nodes**: 41 products\n- **Edges**: Relationships through plant, product group, subgroup, and storage location\n- **Temporal Data**: 221 days of production, sales, factory issues, and deliveries",
   "metadata": {}
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}